{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8b57ee-cf32-498a-af3a-e49121cb250a",
   "metadata": {},
   "source": [
    "# CNN Regularization\n",
    "\n",
    "## ðŸŒŸ What is Regularization in CNNs?\n",
    "Regularization techniques are methods to improve the generalization of a model and prevent overfitting, which occurs when a model performs exceptionally well on training data but poorly on unseen data. Regularization is a critical part of building reliable and robust deep learning models.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Key Concepts and Techniques:\n",
    "### 1. Dropout\n",
    "\n",
    "- Randomly \"drops out\" a fraction of neurons during training, effectively forcing the network to learn redundant representations.\n",
    "- Helps prevent reliance on specific neurons and reduces overfitting.\n",
    "- \n",
    "**Example:** In Keras, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870adb4-1bd0-43b1-b7aa-e1686e10f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "model.add(Dropout(rate=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad10b6-a133-4d23-b250-aaa004b1a8a5",
   "metadata": {},
   "source": [
    "## 2. Weight Decay (L2 Regularization)\n",
    "- Adds a penalty proportional to the magnitude of weights to the loss function, encouraging smaller weights.\n",
    "- Smaller weights help reduce the modelâ€™s complexity, minimizing overfitting.\n",
    "\n",
    "**Implemented in Keras:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3362fe99-a480-40a8-a0d1-13d0611f4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "model.add(Dense(64, kernel_regularizer=l2(0.01)))  # L2 penalty of 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed4800-15f1-4bd7-840e-fdea5f833b1a",
   "metadata": {},
   "source": [
    "### 3. Batch Normalization\n",
    "- Normalizes the input of each layer, ensuring consistent scale across layers during training.\n",
    "- Helps speed up training and makes the model more stable.\n",
    "- Also acts as a regularizer by reducing the modelâ€™s dependency on initial weight distributions.\n",
    "  \n",
    "**Example in Keras:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ecdeba-4ff3-42a5-9716-0037004d844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4a958-acec-4ac8-9a5d-8bbd006830d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2f2a1-6607-4ee7-8072-ebc1ef87e1ae",
   "metadata": {},
   "source": [
    "## ðŸ§  Why Regularization is Necessary for CNNs:\n",
    "\n",
    "- CNNs often have a large number of parameters, making them prone to overfitting.\n",
    "- Techniques like dropout, weight decay, and batch normalization allow CNNs to balance `model complexity` with `generalization ability`, ensuring better performance on test data.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’» Practical Application:\n",
    "- **Dataset:** CIFAR-10 or similar image classification task.\n",
    "- Train a basic CNN without regularization and observe signs of overfitting.\n",
    "    - Example: High accuracy on training data, but low accuracy on test data.\n",
    "- Incorporate regularization techniques and compare model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Week 2: Advanced CNN and Transfer Learning\n",
    "**Focus:**\n",
    "\n",
    "- Dive into complex architectures like ResNet or Inception.\n",
    "- Understand transfer learning, which uses pre-trained models like VGG, ResNet, or Inception to leverage pre-existing knowledge for new tasks.\n",
    "\n",
    "---\n",
    "\n",
    "##ðŸš€ Next Steps:\n",
    "Experiment with varying dropout rates and weight decay values.\n",
    "Train models with and without batch normalization to see its effect on training speed and accuracy.\n",
    "This day sets the foundation for building scalable, efficient, and robust CNN models for complex tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6a6c3-89cc-493b-88a6-788790c7537b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
