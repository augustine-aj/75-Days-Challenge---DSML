{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50aab33-5c92-441d-874b-a881333f7357",
   "metadata": {},
   "source": [
    "# **Hyperparameter Tuning in Deep Learning**\n",
    "\n",
    "## **Objective**\n",
    "Improve the performance of a Convolutional Neural Network (CNN) by finding the best combination of hyperparameters. This involves systematic adjustments to various parameters that control how the model learns.\n",
    "\n",
    "---\n",
    "\n",
    "## **Theory**\n",
    "\n",
    "### **What are Hyperparameters?**\n",
    "- Hyperparameters are settings external to the model that affect the learning process.\n",
    "- Examples include:\n",
    "  - Learning rates\n",
    "  - Batch sizes\n",
    "  - Number of filters\n",
    "  - Activation functions\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Tune Hyperparameters?**\n",
    "- Proper tuning can:\n",
    "  - Significantly improve model performance.\n",
    "  - Prevent overfitting or underfitting.\n",
    "  - Reduce training time.\n",
    "\n",
    "---\n",
    "\n",
    "### **Techniques for Hyperparameter Tuning**\n",
    "\n",
    "#### **Grid Search**\n",
    "- Tests all possible combinations of specified values for hyperparameters.\n",
    "- **Example**: Try different combinations of learning rates `[0.001, 0.01, 0.1]` and batch sizes `[16, 32, 64]`.\n",
    "- **Strength**: Exhaustive and guarantees the best setting.\n",
    "- **Weakness**: Time-consuming for a large parameter space.\n",
    "\n",
    "#### **Random Search**\n",
    "- Samples a fixed number of random combinations from the parameter space.\n",
    "- **Strength**: Faster than grid search and often yields results almost as good.\n",
    "- **Weakness**: No guarantee of finding the best combination.\n",
    "\n",
    "#### **Manual Tuning**\n",
    "- Adjusts hyperparameters based on domain expertise or observations.\n",
    "- **Strength**: Practical for small-scale projects or when intuition plays a role.\n",
    "- **Weakness**: Time-intensive and might miss optimal settings.\n",
    "\n",
    "---\n",
    "\n",
    "## **Practical Implementation**\n",
    "\n",
    "### **1. Hyperparameters to Tune**\n",
    "- **Learning Rate**: Controls the step size during optimization.\n",
    "  - **Too small** → Slow convergence.\n",
    "  - **Too large** → Risk of divergence or overshooting the minima.\n",
    "  \n",
    "- **Batch Size**: Number of samples processed before the model updates weights.\n",
    "  - **Larger batches** → Stable gradients, more memory required.\n",
    "  - **Smaller batches** → Faster updates but noisier gradients.\n",
    "\n",
    "- **Activation Functions**: Affect non-linear transformations in layers. Common options include:\n",
    "  - ReLU\n",
    "  - Sigmoid\n",
    "  - Tanh\n",
    "  - Leaky ReLU\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Steps for Practical Tuning in CNNs**\n",
    "\n",
    "#### **Prepare the Dataset**\n",
    "- Use a dataset like **CIFAR-10** or **Cats vs. Dogs** for image classification.\n",
    "\n",
    "#### **Baseline Model**\n",
    "- Train a CNN with default hyperparameter values to establish baseline accuracy.\n",
    "\n",
    "#### **Tuning Approach**\n",
    "- Use libraries like **GridSearchCV** or **Optuna** for automated tuning.\n",
    "- Perform manual adjustments starting with learning rates and batch sizes.\n",
    "\n",
    "#### **Evaluation**\n",
    "- Use metrics such as:\n",
    "  - Accuracy\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1-score\n",
    "- Track validation loss/accuracy trends to detect overfitting or underfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a72c13-4e9d-461b-971c-a48c936f9472",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23778a9-cb0b-4d3d-ab78-7a4715111726",
   "metadata": {},
   "source": [
    "## Example Code Snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b0240-2f1b-4ae8-b66a-3832b86ea5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6965573-48ea-4be3-a6d6-d917b785141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(learning_rate, activation):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation=activation, input_shape=(32, 32, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation=activation),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662203e5-23c5-40b1-892c-b97b2cf1d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'activation': ['relu', 'tanh']\n",
    "}\n",
    "\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "for params in ParameterGrid(param_grid):\n",
    "    model = build_model(**params)\n",
    "    history = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val), verbose=0)\n",
    "    acc = max(history.history['val_accuracy'])\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd7bf1d-97d4-476e-af54-50d689de93db",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f330ddb3-8396-4cc5-ab5d-c686823548dc",
   "metadata": {},
   "source": [
    "## Key Takeaways:\n",
    "- Tuning hyperparameters like learning rate and batch size can significantly impact model performance.\n",
    "- Automated techniques (grid search, random search) streamline the process for larger parameter spaces.\n",
    "- Always evaluate the tuned model on unseen data to confirm improved generalization.\n",
    "\n",
    "This process is essential to maximize your CNN’s potential while balancing computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed73278d-7a6c-4cb1-b140-4dffe26a3b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
