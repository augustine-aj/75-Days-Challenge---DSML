{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df207b2-e8b5-4ffc-9726-fddcd188e4bd",
   "metadata": {},
   "source": [
    "# Decision Trees: Explanation\n",
    "\n",
    "## üìö Theory\n",
    "\n",
    "A **decision tree** is a supervised learning algorithm used for both classification and regression tasks. It works by splitting data into subsets based on feature values, creating a tree-like structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts:\n",
    "\n",
    "### How Decision Trees Work:\n",
    "1. The **root node** represents the entire dataset.\n",
    "2. **Internal nodes** represent decisions based on feature values.\n",
    "3. **Leaf nodes** represent the final output or prediction.\n",
    "4. The goal is to maximize **homogeneity** in subsets at each split.\n",
    "\n",
    "---\n",
    "\n",
    "### Gini Index:\n",
    "The Gini Index measures the **impurity** or **diversity** in the dataset. It is calculated as:\n",
    "\n",
    "**Gini = 1 ‚àí Œ£·µ¢ (p·µ¢)¬≤**\n",
    "\n",
    "Where:\n",
    "- **p·µ¢** is the probability of a class at a given node.\n",
    "- A Gini Index of **0** indicates perfect purity (all samples belong to one class).\n",
    "\n",
    "---\n",
    "\n",
    "### Information Gain (IG):\n",
    "**Information Gain** measures how much information is gained by making a split. It is calculated as:\n",
    "\n",
    "**IG = Entropy(parent) ‚àí Œ£·µ¢ (|child·µ¢| / |parent| √ó Entropy(child·µ¢))**\n",
    "\n",
    "Where:\n",
    "- **Entropy** measures randomness or uncertainty in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Entropy:\n",
    "Entropy quantifies the randomness in the data and is calculated as:\n",
    "\n",
    "**Entropy = ‚àí Œ£·µ¢ p·µ¢ log‚ÇÇ(p·µ¢)**\n",
    "\n",
    "Where:\n",
    "- **p·µ¢** is the probability of a class at a given node.\n",
    "- Lower entropy indicates greater purity of the subset.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Practical\n",
    "\n",
    "### Building and Visualizing a Decision Tree in Python:\n",
    "Use the **Scikit-learn** library to build and visualize a decision tree for classification tasks. Example steps include:\n",
    "1. Preprocess the dataset.\n",
    "2. Train a decision tree classifier.\n",
    "3. Visualize the tree using `sklearn.tree.plot_tree`.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Metrics:\n",
    "\n",
    "### 1. Accuracy:\n",
    "The proportion of correct predictions out of all predictions made:\n",
    "**Accuracy = (TP + TN) / (TP + TN + FP + FN)**\n",
    "\n",
    "### 2. Confusion Matrix:\n",
    "A matrix summarizing prediction results:\n",
    "- **True Positives (TP):** Correctly predicted positives.\n",
    "- **True Negatives (TN):** Correctly predicted negatives.\n",
    "- **False Positives (FP):** Incorrectly predicted positives.\n",
    "- **False Negatives (FN):** Incorrectly predicted negatives.\n",
    "\n",
    "Decision trees are interpretable and powerful for structured data, though they can overfit without proper pruning or regularization techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040c2bcd-ffd3-487c-86fc-995466381a6e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3d2132-fdd9-4665-b406-371a7e9c4f34",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f548a62-a64f-485f-986a-c214344cfc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"sample_dataset.csv\")\n",
    "X = data.drop(\"target\", axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(model, feature_names=X.columns, class_names=str(model.classes_), filled=True)\n",
    "plt.show()\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2681196-cd76-402c-901f-a3b666918c9c",
   "metadata": {},
   "source": [
    "## Explanation of the Code:\n",
    "### Data Preprocessing:\n",
    "\n",
    "- ***drop(\"target\", axis=1):*** Separates features and target labels.\n",
    "- ***train_test_split:*** Splits the dataset into training and testing sets.\n",
    "### Model Training:\n",
    "\n",
    "- ***DecisionTreeClassifier:*** Builds the tree using the Gini Index as the splitting criterion.\n",
    "- ***max_depth:*** Limits the tree's depth to prevent overfitting.\n",
    "### Visualization:\n",
    "\n",
    "- ***plot_tree:*** Generates a graphical representation of the decision tree, showing how splits are made.\n",
    "### Model Evaluation:\n",
    "\n",
    "- ***accuracy_score:*** Computes the proportion of correctly classified samples.\n",
    "- ***confusion_matrix:*** Displays the matrix with true/false positives and negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9663f96-9165-43b0-a105-b88a419b2da7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df86d3-bcd0-4894-b291-080a3936dacd",
   "metadata": {},
   "source": [
    "## Output Example\n",
    "Decision Tree Plot: Shows feature splits and leaf nodes with class predictions.\n",
    "- **Confusion Matrix:**\n",
    "**Accuracy = (TP + TN) / (TP + TN + FP + FN)**\n",
    "  \n",
    "- ***Accuracy:*** A single value indicating the prediction success rate.\n",
    "This process demonstrates the complete workflow of building, visualizing, and evaluating a decision tree model in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d62473-7bc4-4246-bbae-abc4ca40ac7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
