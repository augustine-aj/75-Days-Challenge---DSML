{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1fd3ff2-7acf-464f-80f3-d8c151d412d2",
   "metadata": {},
   "source": [
    "# Supervised Learning - Model Evaluation\n",
    "\n",
    "Model evaluation is a critical step in understanding how well a machine learning model performs on unseen data. It involves using various metrics to assess the model's accuracy and reliability.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Metrics\n",
    "\n",
    "#### Accuracy\n",
    "The ratio of correct predictions to total predictions.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **TP**: True Positives  \n",
    "- **TN**: True Negatives  \n",
    "- **FP**: False Positives  \n",
    "- **FN**: False Negatives  \n",
    "\n",
    "---\n",
    "\n",
    "#### Precision\n",
    "The ratio of true positives to the total predicted positives.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Recall (Sensitivity)\n",
    "The ratio of true positives to the total actual positives.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### F1-Score\n",
    "The harmonic mean of precision and recall.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ROC Curve (Receiver Operating Characteristic)\n",
    "Plots the **True Positive Rate (TPR)** against the **False Positive Rate (FPR)** at various threshold values.\n",
    "\n",
    "- **True Positive Rate (TPR)**:\n",
    "$$\n",
    "\\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "- **False Positive Rate (FPR)**:\n",
    "$$\n",
    "\\frac{FP}{FP + TN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### AUC (Area Under Curve)\n",
    "The **AUC** measures the ability of the model to distinguish between classes. A higher AUC indicates better performance:\n",
    "- **AUC = 1.0**: Perfect model.\n",
    "- **AUC = 0.5**: Random guessing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f0688b-1a30-4234-922c-c2fa60cdecf9",
   "metadata": {},
   "source": [
    "## Practical: Evaluate Models in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78245c00-fbb8-4b18-a1c1-fc46d81182b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"sample_dataset.csv\")  # Replace with your dataset\n",
    "X = data.drop(\"target\", axis=1)\n",
    "y = data[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]  # Probabilities for ROC curve\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c94487b-5365-42c2-be08-ce80e77d6fe6",
   "metadata": {},
   "source": [
    "### Explanation of the Code\n",
    "- **Model Training:**\n",
    "\n",
    "    - Trains a decision tree model on the dataset.\n",
    "- **Classification Report:**\n",
    "\n",
    "    - `classification_report`: Provides a summary of precision, recall, F1-score, and accuracy for each class.\n",
    "- **ROC Curve:**\n",
    "\n",
    "    - `roc_curve:` Computes the FPR and TPR for different thresholds.\n",
    "    - `auc:` Calculates the area under the ROC curve.\n",
    "    - Plots the ROC curve, highlighting the trade-off between sensitivity and specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7cfa24-3754-4a2a-8e1a-4d6899ee949a",
   "metadata": {},
   "source": [
    "### Example Output\n",
    "#### Classification Report:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ef97a1-64e6-4c4b-806d-34e12783d40a",
   "metadata": {},
   "source": [
    "| Metric      | Class 0 | Class 1 | Weighted Avg |\n",
    "|-------------|---------|---------|--------------|\n",
    "| Precision   | 0.85    | 0.90    | 0.87         |\n",
    "| Recall      | 0.80    | 0.92    | 0.85         |\n",
    "| F1-Score    | 0.82    | 0.91    | 0.86         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef04293-9102-445c-a533-4f6cbba8796f",
   "metadata": {},
   "source": [
    "**ROC Curve:**\n",
    "- A graph with TPR (True Positive Rate) on the Y-axis and FPR (False Positive Rate) on the X-axis.\n",
    "- AUC = 0.95 (indicating excellent model performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0451ae7e-dcb4-4fb1-bec7-98061274e2d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca13156-f0a2-4989-a52f-04096ad4ef64",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Accuracy works well with balanced datasets.\n",
    "- Precision and Recall are crucial for imbalanced datasets, focusing on relevant and missed predictions, respectively.\n",
    "- F1-Score balances precision and recall.\n",
    "- ROC Curve and AUC help understand the trade-off between sensitivity and specificity.\n",
    "This evaluation workflow ensures a comprehensive understanding of model performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7d335-e244-445d-ae6c-4296db74a9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
